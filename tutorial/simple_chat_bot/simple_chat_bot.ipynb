{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Chat Bot\n",
    "This chatbot that built only use the language model. It will related concepts:\n",
    "- Conversational RAG: enable chat bot with external source data\n",
    "- Agents: chat bot can take actions\n",
    "Concept:\n",
    "- Chat Model: chatbot interface is based around message rather than raw text, Chat Model is best suited.\n",
    "- Prompt Templates\n",
    "- Chat History\n",
    "- LangSmith to debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "model = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AI: Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])\n",
    "# so you can re-check the logs in langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bob! According to our conversation, your name is indeed \"Bob\"!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "  [\n",
    "  HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "  AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "  HumanMessage(content=\"What's my name?\"),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. message history\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "  if session_id not in store:\n",
    "    store[session_id] = ChatMessageHistory()\n",
    "  return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_llm_end callback: KeyError('message')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"AI: Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but now you need to create a new session id\n",
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "  [HumanMessage(content=\"Hi! I'm Bob\")],\n",
    "  config=config,\n",
    ")\n",
    "print(response)\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Bob! It's great to meet you! How can I assist you today? Do you have any specific questions or topics you'd like to discuss? I'm here to help with anything from answering general knowledge questions, providing information on a particular topic, offering suggestions, or just chatting. Let me know what's on your mind, and I'll do my best to be helpful!\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Prompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\"messages\": [HumanMessage(content=\"hi! I'm bob\")]})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Jim! Nice to meet you. I'm here to help with any questions or tasks you may have. What's on your mind today? Do you need assistance with something specific, or just looking for some general information or ideas? Let me know and I'll do my best to assist you!\n",
      "Jim! You've introduced yourself as Jim, so that's correct! If you ever forget or want to double-check, feel free to ask me anytime. Now, what else can I help you with today? Do you have a specific question or topic in mind, or would you like some suggestions or ideas?\n"
     ]
    }
   ],
   "source": [
    "# can now wrap this in the same Messages History object as before\n",
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)\n",
    "config = {\"configurable\": {\"session_id\": \"abc5\"}}\n",
    "response = with_message_history.invoke(\n",
    "  [HumanMessage(content=\"Hi! I'm Jim\")],\n",
    "  config=config,\n",
    ")\n",
    "print(response)\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola Todd! Soy tu asistente amable y estoy aquí para ayudarte en lo que necesites. ¿En qué puedo ayudarte hoy?\n",
      "¡Hola Todd! ¡Eso es fácil! Tu nombre es Todd, ¿correcto?\n"
     ]
    }
   ],
   "source": [
    "# again, with the prompt template and the model\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\n",
    "      \"system\",\n",
    "      \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "  ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "# response = chain.invoke(\n",
    "#   {\"messages\": [HumanMessage(content=\"hi! I'm bob\")], \"language\": \"Spanish\"}\n",
    "# )\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "  chain,\n",
    "  get_session_history,\n",
    "  input_messages_key=\"messages\",\n",
    ")\n",
    "config = {\"configurable\": {\"session_id\": \"abc11\"}}\n",
    "response = with_message_history.invoke(\n",
    "  {\"messages\": [HumanMessage(content=\"hi! I'm todd\")], \"language\": \"Spanish\"},\n",
    "  config=config,\n",
    ")\n",
    "print(response)\n",
    "response = with_message_history.invoke(\n",
    "  {\"messages\": [HumanMessage(content=\"whats my name?\")], \"language\": \"Spanish\"},\n",
    "  config=config,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Manage conversation history\n",
    "# Painpoint: If unmanaged, the conversation history can grow indefinitely, consuming memory and sizing.\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# example to filter with the last 10 messages\n",
    "def filter_messages(messages, k=10):\n",
    "  return messages[-k:]\n",
    "\n",
    "chain = (\n",
    "  RunnablePassthrough.assign(messages=lambda x: filter_messages(x[\"messages\"]))\n",
    "  | prompt\n",
    "  | model\n",
    ")\n",
    "messages = [\n",
    "  HumanMessage(content=\"hi! I'm bob\"),\n",
    "  AIMessage(content=\"hi!\"),\n",
    "  HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "  AIMessage(content=\"nice\"),\n",
    "  HumanMessage(content=\"whats 2 + 2\"),\n",
    "  AIMessage(content=\"4\"),\n",
    "  HumanMessage(content=\"thanks\"),\n",
    "  AIMessage(content=\"no problem!\"),\n",
    "  HumanMessage(content=\"having fun?\"),\n",
    "  AIMessage(content=\"yes!\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm happy to help!\\n\\nUnfortunately, I don't have any information about your name. You didn't introduce yourself or provide any identifying information. If you'd like, we can start fresh and explore more topics together! What would you like to talk about or ask next?\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now try it out! If we create a list of messages more than 10 messages long, we can see what it no longer remembers information in the early messages.\n",
    "response = chain.invoke(\n",
    "  {\n",
    "    \"messages\": messages + [HumanMessage(content=\"what's my name?\")],\n",
    "    \"language\": \"English\",\n",
    "  }\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You already told me that your favorite ice cream is vanilla!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but if we ask about information that is within the last ten messages, it still remembers it\n",
    "response = chain.invoke(\n",
    "  {\n",
    "    \"messages\": messages + [HumanMessage(content=\"what's my fav ice cream\")],\n",
    "    \"language\": \"English\",\n",
    "  }\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap this in the Message History\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "  chain,\n",
    "  get_session_history,\n",
    "  input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc20\"}}\n",
    "response = with_message_history.invoke(\n",
    "  {\n",
    "    \"messages\": [HumanMessage(content=\"whats my favorite ice cream?\")],\n",
    "    \"language\": \"English\",\n",
    "  },\n",
    "  config=config,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice| to| meet| you|,| Todd|!\n",
      "\n",
      "|Here|'s| one|:\n",
      "\n",
      "|Why| don|'t| scientists| trust| atoms|?\n",
      "\n",
      "|Because| they| make| up| everything|!\n",
      "\n",
      "|Hope| that| made| you| gig|gle|!| Do| you| want| another| one|?||"
     ]
    }
   ],
   "source": [
    "# Implement with streaming\n",
    "config = {\"configurable\": {\"session_id\": \"abc15\"}}\n",
    "for r in with_message_history.stream(\n",
    "  {\n",
    "    \"messages\": [HumanMessage(content=\"hi! I'm todd. tell me a joke\")],\n",
    "    \"language\": \"English\",\n",
    "  },\n",
    "  config=config,\n",
    "):\n",
    "  print(r, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice| to| meet| you| too|,| Todd|!\n",
      "\n",
      "|As| for| your| question|,| I|'d| be| happy| to| remind| you| -| your| name| is| Todd|!\n",
      "\n",
      "|And|,| would| you| like| to| hear| the| joke| again|?\n",
      "|Why| don|'t| scientists| trust| atoms|?| Because| they| make| up| everything|!| Hope| that| made| you| smile| again|!||"
     ]
    }
   ],
   "source": [
    "for r in with_message_history.stream(\n",
    "  {\n",
    "    \"messages\": [HumanMessage(content=\"What is my name?, and tell me joke that you said before\")],\n",
    "    \"language\": \"English\",\n",
    "  },\n",
    "  config=config,\n",
    "):\n",
    "  print(r, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
