{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to stream runnables\n",
    "Streaming is critical in making applications based on LLMs feel responsive to end-users. This interface provider 2 general approches:\n",
    "1. sync `stream` and async `astream`: a default implementation of streaming that streams the final output from the chain.\n",
    "2. async `astream_events` and async `astream_log`: these provide a way to stream both intermediate steps and final output from the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "model = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Stream\n",
    "NOTE: select from working environment it meant if model streaming run on async function, you should use `astream` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The| sky| appears| blue| to| our| eyes| during| the| daytime|,| but| it| can| take| on| hues| of| red|,| orange|,| and| purple| during| sunrise| and| sunset|.||"
     ]
    }
   ],
   "source": [
    "# sync stream API:\n",
    "chunks = []\n",
    "for chunk in model.stream(\"what color is the sky? Answer in short sentence.\"):\n",
    "  chunks.append(chunk)\n",
    "  print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The| sky| appears| blue| to| our| eyes|,| but| it| can| also| appear| other| colors| depending| on| the| time| of| day| and| atmospheric| conditions|.||"
     ]
    }
   ],
   "source": [
    "# async astream API:\n",
    "chunks = []\n",
    "async for chunk in model.astream(\"what color is the sky? Answer in short sentence.\"):\n",
    "  chunks.append(chunk)\n",
    "  print(chunk, end=\"|\", flush=True)\n",
    "  # print(chuck.content, end=\"|\", flush=True) # depending on models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]\n",
    "# Some model, you can got back something called AIMessageChunk\n",
    "# from langchain_core.messages.ai import AIMessageChunk\n",
    "# AIMessageChunk(content=\"The\", id='run-b36bea64-5511-4d7a-b6a3-a07b3db0c8e7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here|'s| one|:\n",
      "\n",
      "|Why| did| the| par|rot| go| to| the| doctor|?\n",
      "\n",
      "|Because| it| had| a| f|owl| temper|!\n",
      "\n",
      "|Hope| that| made| you| squ|awk| with| laughter|!||"
     ]
    }
   ],
   "source": [
    "# Chains\n",
    "# involves more step using LangChain Expression Language (LCEL): combines a prompt, model and a parser and verify that streaming works\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "async for chunk in chain.astream({\"topic\": \"parrot\"}):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'countries': []}\n",
      "{'countries': [{}]}\n",
      "{'countries': [{'name': ''}]}\n",
      "{'countries': [{'name': 'France'}]}\n",
      "{'countries': [{'name': 'France', 'population': 653}]}\n",
      "{'countries': [{'name': 'France', 'population': 653451}]}\n",
      "{'countries': [{'name': 'France', 'population': 65345100}]}\n",
      "{'countries': [{'name': 'France', 'population': 65345100}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 65345100}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 65345100}, {'name': 'Spain'}]}\n",
      "{'countries': [{'name': 'France', 'population': 65345100}, {'name': 'Spain', 'population': 467}]}\n",
      "{'countries': [{'name': 'France', 'population': 65345100}, {'name': 'Spain', 'population': 467527}]}\n",
      "{'countries': [{'name': 'France', 'population': 65345100}, {'name': 'Spain', 'population': 46752716}]}\n",
      "{'countries': [{'name': 'France', 'population': 65345100}, {'name': 'Spain', 'population': 46752716}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 65345100}, {'name': 'Spain', 'population': 46752716}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 65345100}, {'name': 'Spain', 'population': 46752716}, {'name': 'Japan'}]}\n",
      "{'countries': [{'name': 'France', 'population': 65345100}, {'name': 'Spain', 'population': 46752716}, {'name': 'Japan', 'population': 128}]}\n",
      "{'countries': [{'name': 'France', 'population': 65345100}, {'name': 'Spain', 'population': 46752716}, {'name': 'Japan', 'population': 128000}]}\n",
      "{'countries': [{'name': 'France', 'population': 65345100}, {'name': 'Spain', 'population': 46752716}, {'name': 'Japan', 'population': 128000000}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "chain = model | JsonOutputParser()  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\n",
    "async for text in chain.astream(\n",
    "  \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "  'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "  \"Each country should have the key `name` and `population`\"\n",
    "):\n",
    "  print(text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France', 'Spain', 'Japan']|"
     ]
    }
   ],
   "source": [
    "# Any steps in the chain that operate on finalized inputs rather than on input streams can break streaming functionality via stream or astream.\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# A function that operates on finalized inputs\n",
    "# Streaming will not work with this function\n",
    "def _extract_country_names(inputs):\n",
    "    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n",
    "    if not isinstance(inputs, dict):\n",
    "        return \"\"\n",
    "    if \"countries\" not in inputs:\n",
    "        return \"\"\n",
    "    countries = inputs[\"countries\"]\n",
    "\n",
    "    if not isinstance(countries, list):\n",
    "        return \"\"\n",
    "    country_names = [\n",
    "        country.get(\"name\") for country in countries if isinstance(country, dict)\n",
    "    ]\n",
    "    return country_names\n",
    "\n",
    "\n",
    "chain = model | JsonOutputParser() | _extract_country_names\n",
    "\n",
    "async for text in chain.astream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\"\n",
    "):\n",
    "    print(text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France|Spain|Japan|"
     ]
    }
   ],
   "source": [
    "# Solve the problem using generator functions\n",
    "# generator functions (yield): a function that returns an iterator and can be paused and resumed\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "async def _extract_country_names_streaming(input_stream):\n",
    "  \"\"\"A function that operates on input streams.\"\"\"\n",
    "  country_names_so_far = set()\n",
    "\n",
    "  async for input in input_stream:\n",
    "    if not isinstance(input, dict):\n",
    "      continue\n",
    "\n",
    "    if \"countries\" not in input:\n",
    "      continue\n",
    "\n",
    "    countries = input[\"countries\"]\n",
    "\n",
    "    if not isinstance(countries, list):\n",
    "      continue\n",
    "\n",
    "    for country in countries:\n",
    "      name = country.get(\"name\")\n",
    "      if not name:\n",
    "        continue\n",
    "      if name not in country_names_so_far:\n",
    "        yield name\n",
    "        country_names_so_far.add(name)\n",
    "\n",
    "chain = model | JsonOutputParser() | _extract_country_names_streaming\n",
    "\n",
    "async for text in chain.astream(\n",
    "  \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "  'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "  \"Each country should have the key `name` and `population`\",\n",
    "):\n",
    "  print(text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(page_content='harrison likes spicy food'),\n",
       "  Document(page_content='harrison worked at kensho')]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Non-streaming components\n",
    "# some built-in components like Retriever, which fetches data from the internet, do not support streaming\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "# pip install faiss-gpu or pip install faiss-cpu\n",
    "vectorstore = FAISS.from_texts(\n",
    "  [\"harrison worked at kensho\", \"harrison likes spicy food\"],\n",
    "  # embedding=OpenAIEmbeddings(),\n",
    "  embedding=OllamaEmbeddings(model=\"llama3\"),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "chunks = [chunk for chunk in retriever.stream(\"where did harrison work?\")]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based| on| the| given| context|,| Harrison| worked| at| Kens|ho|.\n",
      "\n",
      "|Here| are| three| made|-up| sentences| about| Kens|ho|:\n",
      "\n",
      "|K|ens|ho| is| a| trendy| restaurant| that| serves| an| array| of| international| dishes| with| a| focus| on| bold| flavors| and| spices|.| The| vibrant| atmosphere| and| eclectic| decor| make| it| a| popular| spot| for| food|ies| and| social|ites| alike|.| As| the| go|-to| place| for| spicy| food| enthusiasts|,| Kens|ho|'s| chefs| are| always| experimenting| with| new| hot| sauces| and| marin|ades| to| tantal|ize| taste| buds|.||"
     ]
    }
   ],
   "source": [
    "# But we can use RunnablePassthrough to convert the non-streaming component to a streaming component\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "retrieval_chain = (\n",
    "  {\n",
    "    \"context\": retriever.with_config(run_name=\"Docs\"),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "  }\n",
    "  | prompt\n",
    "  | model\n",
    "  | StrOutputParser()\n",
    ")\n",
    "for chunk in retrieval_chain.stream(\"Where did harrison work? \" \"Write 3 made up sentences about this place.\"):\n",
    "  print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
